# GroupedQueryAttention
Unofficial backup repo. for GQA

Original paper: https://arxiv.org/pdf/2305.13245.pdf
@misc{ainslie2023gqa,
      title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints}, 
      author={Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebr√≥n and Sumit Sanghai},
      year={2023},
      eprint={2305.13245},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
